{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(col):\n",
    "    q1, q3 = col.quantile([0.25, 0.75])\n",
    "    IQR = q3 - q1\n",
    "    lwr_bound = q1 - (1.5 * IQR)\n",
    "    upr_bound = q3 + (1.5 * IQR)\n",
    "    return lwr_bound, upr_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr= pd.read_csv('Final_Dataset_after_temperature.csv')\n",
    "temp=pd.read_csv('Final_Dataset_after_temperature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-12340.5 20879.5\n"
     ]
    }
   ],
   "source": [
    "low, high = remove_outlier(ytr[\"Production_in_tons\"])\n",
    "print(low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143414"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytr=ytr[ytr['Production_in_tons']<=high]\n",
    "len(ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values above 20879.5: 0\n"
     ]
    }
   ],
   "source": [
    "threshold = 20879.5\n",
    "count_above_threshold = ytr[ytr['Production_in_tons'] > threshold].shape[0]\n",
    "\n",
    "print(f\"Number of values above {threshold}: {count_above_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=ytr.drop([\"Yield_ton_per_hec\", \"Production_in_tons\"],axis=1)\n",
    "y=ytr[\"Production_in_tons\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_encoded=pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 10,  # Reduced max_depth to avoid overfitting\n",
    "    'learning_rate': 0.1,  # Reduced learning rate to improve accuracy\n",
    "    'n_estimators': 1000,\n",
    "    'subsample': 0.8,  # To avoid overfitting\n",
    "    'colsample_bytree': 0.8,  # To avoid overfitting\n",
    "    'eval_metric': 'rmse',\n",
    "    'early_stopping_rounds': 10  # Stops if no improvement over 10 rounds\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaire\\anaconda3\\lib\\site-packages\\xgboost\\core.py:158: UserWarning: [12:18:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0015a694724fa8361-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"early_stopping_rounds\", \"n_estimators\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "model_xgb = xgb.train(params, dtrain, num_boost_round=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = model_xgb.predict(dtest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Model - Mean Squared Error: 2630684.7686622743\n",
      "XGBoost Model - Mean Absolute Error: 681.5307730484164\n",
      "XGBoost Model - R² Score: 0.8502684248224954\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score,accuracy_score\n",
    "\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f'XGBoost Model - Mean Squared Error: {mse_xgb}')\n",
    "print(f'XGBoost Model - Mean Absolute Error: {mae_xgb}')\n",
    "print(f'XGBoost Model - R² Score: {r2_xgb}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# west bengal,rabi,Sesamum,152.54000000000002,22.28,244.0,95.0,0.38934426229508196\n",
    "custom_input = {\n",
    "    'State_Name': 'west bengal',\n",
    "    'Crop_Type': 'rabi',\n",
    "    'Crop': 'Sesamum',\n",
    "    'rainfall': 152.54000000000002,\n",
    "    'temperature': 22.28,\n",
    "    'Area_in_hectares': 244.0,\n",
    "}\n",
    "custom_input_df = pd.DataFrame([custom_input])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rainfall  temperature  Area_in_hectares  \\\n",
      "0    152.54        22.28             244.0   \n",
      "\n",
      "   State_Name_andaman and nicobar islands  State_Name_andhra pradesh  \\\n",
      "0                                       0                          0   \n",
      "\n",
      "   State_Name_arunachal pradesh  State_Name_assam  State_Name_bihar  \\\n",
      "0                             0                 0                 0   \n",
      "\n",
      "   State_Name_chandigarh  State_Name_chhattisgarh  ...  Crop_Turmeric  \\\n",
      "0                      0                        0  ...              0   \n",
      "\n",
      "   Crop_Turnip  Crop_Urad  Crop_Varagu  Crop_Water Melon  Crop_Wheat  \\\n",
      "0            0          0            0                 0           0   \n",
      "\n",
      "   Crop_Yam  Crop_other fibres  Crop_other misc. pulses  Crop_other oilseeds  \n",
      "0         0                  0                        0                    0  \n",
      "\n",
      "[1 rows x 154 columns]\n"
     ]
    }
   ],
   "source": [
    "custom_input_encoded = pd.get_dummies(custom_input_df)\n",
    "\n",
    "# # Align the custom input with the training set to ensure it has the same columns\n",
    "custom_input_encoded = custom_input_encoded.reindex(columns=X_encoded.columns, fill_value=0)\n",
    "print(custom_input_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dmatrix = xgb.DMatrix(custom_input_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Production in tons: 96.93362426757812\n"
     ]
    }
   ],
   "source": [
    "y_pred_custom = model_xgb.predict(custom_dmatrix)\n",
    "print(f'Predicted Production in tons: {y_pred_custom[0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Production in tons: 699.2440795898438\n"
     ]
    }
   ],
   "source": [
    "def pred(model,input_data):\n",
    "    custom_input_df = pd.DataFrame([input_data])\n",
    "    custom_input_encoded = pd.get_dummies(custom_input_df)\n",
    "\n",
    "# # Align the custom input with the training set to ensure it has the same columns\n",
    "    custom_input_encoded = custom_input_encoded.reindex(columns=X_encoded.columns, fill_value=0)\n",
    "    custom_dmatrix = xgb.DMatrix(custom_input_encoded)\n",
    "    y_pred_custom = model_xgb.predict(custom_dmatrix)\n",
    "    print(f'Predicted Production in tons: {y_pred_custom[0]}')\n",
    "custom_input = {\n",
    "    'State_Name': 'andhra pradesh',\n",
    "    'Crop_Type': 'kharif',\n",
    "    'Crop': 'Arhar/Tur',\n",
    "    'rainfall': 654.34,\n",
    "    'temperature': 29.27,\n",
    "    'Area_in_hectares': 1400,\n",
    "}\n",
    "pred(model_xgb,custom_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\kaire\\anaconda3\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\kaire\\anaconda3\\lib\\site-packages (from lightgbm) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\kaire\\anaconda3\\lib\\site-packages (from lightgbm) (1.24.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kaire\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:204: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Provided parameters constrain tree depth (max_depth=10) without explicitly setting 'num_leaves'. This can lead to underfitting. To resolve this warning, pass 'num_leaves' (<=1024) in params. Alternatively, pass (max_depth=-1) and just use 'num_leaves' to constrain model complexity.\n",
      "[LightGBM] [Warning] Provided parameters constrain tree depth (max_depth=10) without explicitly setting 'num_leaves'. This can lead to underfitting. To resolve this warning, pass 'num_leaves' (<=1024) in params. Alternatively, pass (max_depth=-1) and just use 'num_leaves' to constrain model complexity.\n",
      "[LightGBM] [Warning] Provided parameters constrain tree depth (max_depth=10) without explicitly setting 'num_leaves'. This can lead to underfitting. To resolve this warning, pass 'num_leaves' (<=1024) in params. Alternatively, pass (max_depth=-1) and just use 'num_leaves' to constrain model complexity.\n",
      "[LightGBM] [Warning] Provided parameters constrain tree depth (max_depth=10) without explicitly setting 'num_leaves'. This can lead to underfitting. To resolve this warning, pass 'num_leaves' (<=1024) in params. Alternatively, pass (max_depth=-1) and just use 'num_leaves' to constrain model complexity.\n",
      "[LightGBM] [Warning] Provided parameters constrain tree depth (max_depth=10) without explicitly setting 'num_leaves'. This can lead to underfitting. To resolve this warning, pass 'num_leaves' (<=1024) in params. Alternatively, pass (max_depth=-1) and just use 'num_leaves' to constrain model complexity.\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002759 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 733\n",
      "[LightGBM] [Info] Number of data points in the train set: 114731, number of used features: 131\n",
      "[LightGBM] [Info] Start training from score 2416.933517\n",
      "Training until validation scores don't improve for 10 rounds\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Early stopping, best iteration is:\n",
      "[968]\ttraining's rmse: 1386.2\tvalid_1's rmse: 1532.83\n",
      "LightGBM Model - Mean Squared Error: 2349574.2922685314\n",
      "LightGBM Model - Mean Absolute Error: 697.6537405736528\n",
      "LightGBM Model - R² Score: 0.8662684849326003\n",
      "Predicted Production in tons: 201.41211843406572\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Train LightGBM model\n",
    "model_lgb = lgb.train(params, train_data, valid_sets=[train_data, test_data], num_boost_round=1000)\n",
    "\n",
    "# Predictions\n",
    "y_pred_lgb = model_lgb.predict(X_test, num_iteration=model_lgb.best_iteration)\n",
    "\n",
    "# Evaluate LightGBM model\n",
    "mse_lgb = mean_squared_error(y_test, y_pred_lgb)\n",
    "mae_lgb = mean_absolute_error(y_test, y_pred_lgb)\n",
    "r2_lgb = r2_score(y_test, y_pred_lgb)\n",
    "\n",
    "print(f'LightGBM Model - Mean Squared Error: {mse_lgb}')\n",
    "print(f'LightGBM Model - Mean Absolute Error: {mae_lgb}')\n",
    "print(f'LightGBM Model - R² Score: {r2_lgb}')\n",
    "\n",
    "# Prediction function\n",
    "def pred(model, input_data):\n",
    "    custom_input_df = pd.DataFrame([input_data])\n",
    "    custom_input_encoded = pd.get_dummies(custom_input_df)\n",
    "    \n",
    "    # Align the custom input with the training set to ensure it has the same columns\n",
    "    custom_input_encoded = custom_input_encoded.reindex(columns=X_encoded.columns, fill_value=0)\n",
    "    \n",
    "    # Predict using LightGBM\n",
    "    y_pred_custom = model.predict(custom_input_encoded, num_iteration=model.best_iteration)\n",
    "    print(f'Predicted Production in tons: {y_pred_custom[0]}')\n",
    "\n",
    "# Custom input example\n",
    "custom_input = {\n",
    "    'State_Name': 'west bengal',\n",
    "    'Crop_Type': 'rabi',\n",
    "    'Crop': 'Sesamum',\n",
    "    'rainfall': 152.54000000000002,\n",
    "    'temperature': 22.28,\n",
    "    'Area_in_hectares': 244.0,\n",
    "}\n",
    "\n",
    "# Predict on custom input using LightGBM\n",
    "pred(model_lgb, custom_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
